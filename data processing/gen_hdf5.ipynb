{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate hdf45 from *.raw and *.dat based on preprocess type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file https://dataset.prophesee.ai/index.php/s/YAri3vpPZHhEZfc/download -> .\\spinner.dat\n",
      "...99%, 413 MB, 8486 KB/s, 49 seconds passedspinner.dat\n",
      ".\\spinner.h5\n",
      "Processing spinner.dat, storing results in .\\spinner.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:03,  6.44it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "from metavision_ml.preprocessing.viz import filter_outliers\n",
    "from metavision_ml.preprocessing.hdf5 import generate_hdf5\n",
    "from metavision_core.utils import get_sample\n",
    "\n",
    "\n",
    "input_path = \"spinner.dat\"\n",
    "get_sample(input_path, '.')\n",
    "print(os.path.basename(input_path))\n",
    "# if the file doesn't exist, it will be downloaded from Prophesee's public sample server\n",
    "\n",
    "\n",
    "# get_sample(input_path, folder=\".\")\n",
    "\n",
    "output_folder = \".\"\n",
    "output_path = output_folder + os.sep + os.path.basename(input_path).replace('.dat', '.h5')\n",
    "print(output_path)\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    generate_hdf5(paths=input_path, output_folder=output_folder, preprocess=\"timesurface\", delta_t=250000, height=None, width=None,\n",
    "              start_ts=0, max_duration=None)\n",
    "    \n",
    "    \n",
    "# print(f'\\nOriginal file {input_path} is of size: {os.path.getsize(input_path)/1e6}MB')\n",
    "# print(f'\\nResult file {output_path} is of size: {os.path.getsize(output_path)/1e6}MB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presence of datas in converted hdf5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (21, 2, 480, 640) ----> total events(0.25s), polarity, height, width\n",
      "delta_t: 250000\n",
      "event_input_height: 480\n",
      "event_input_width: 640\n",
      "events_to_tensor: b'timesurface'\n",
      "mode: delta_t\n",
      "n_events: 0\n",
      "shape: [  2 480 640]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import h5py\n",
    "\n",
    "file = h5py.File(output_path, 'r')\n",
    "print(f\"shape: {file['data'].shape} ----> total events(0.25s), polarity, height, width\")\n",
    "\n",
    "for keys, value in file['data'].attrs.items():\n",
    "    print(f\"{keys}: {value}\")\n",
    "\n",
    "# Note that the HDF5 dataset variable is similar to a numpy 'ndarray' but has some unique features. \n",
    "# An important difference is that, if you read from an HDF5 dataset, \n",
    "# the data is actually read from drive and put to memory as a numpy array. \n",
    "# If you are handling a large dataset,it is recommended not to read the whole file all at once,\n",
    "# to avoid saturating the memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "events_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
